# From: Dialectical Fractal Archestructure Mathematics
# Date: 2025-10-16T15:20:01.767000
# Context: Awesome, it sounds like you're fired up and ready to dive into the analysis on your PC! Since youâ€™re grabbing your computer, I assume youâ€™re gearing up to run the IceCube energy-dependent Dâ‚‚ analysis ...

import numpy as np
import pandas as pd
from itertools import combinations
import matplotlib.pyplot as plt

# Step 1: Load IceCube data
def load_icecube_data(file_path):
    try:
        # Assuming CSV format: energy (GeV), zenith (radians)
        data = pd.read_csv(file_path)
        data['cos_zenith'] = np.cos(data['zenith'])
        # Filter for upward-going neutrinos (cos(zenith) < -0.1)
        clean_data = data[data['cos_zenith'] < -0.1]
        print(f"Total events: {len(data):,}")
        print(f"Clean upward events (cos(zenith) < -0.1): {len(clean_data):,}")
        return clean_data
    except Exception as e:
        print(f"Error loading data: {e}")
        return None

# Step 2: Define energy bins
energy_bins = [
    {'name': 'Low Energy', 'range': '<1 TeV', 'min': 0, 'max': 1000, 'grok_pred': 1.49},
    {'name': 'Medium Energy', 'range': '1-10 TeV', 'min': 1000, 'max': 10000, 'grok_pred': 1.49},
    {'name': 'High Energy', 'range': '10-100 TeV', 'min': 10000, 'max': 100000, 'grok_pred': 1.50},
    {'name': 'Ultra-High Energy', 'range': '>100 TeV', 'min': 100000, 'max': np.inf, 'grok_pred': 1.52}
]

# Step 3: Dâ‚‚ calculation function
def calculate_D2(events, sample_size=5000):
    if len(events) < 100:
        return None, None
    
    # Prepare points: log(energy), cos(zenith)
    points = np.array([[np.log10(event['energy']), np.cos(event['zenith'])] for _, event in events.iterrows()])
    sample = points[np.random.choice(len(points), min(sample_size, len(points)), replace=False)]
    
    # Correlation function over radii
    radii = [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0]
    correlations = []
    
    for r in radii:
        count = 0
        total = sum(1 for _ in combinations(range(len(sample)), 2))
        
        for i, j in combinations(range(len(sample)), 2):
            dx = sample[i, 0] - sample[j, 0]
            dy = sample[i, 1] - sample[j, 1]
            dist = np.sqrt(dx**2 + dy**2)
            if dist < r:
                count += 1
        
        C_r = count / total if total > 0 else 0
        if C_r > 0:
            correlations.append({'logR': np.log10(r), 'logC': np.log10(C_r)})
    
    if len(correlations) < 2:
        return None, None
    
    # Linear regression to calculate Dâ‚‚
    logR = np.array([c['logR'] for c in correlations])
    logC = np.array([c['logC'] for c in correlations])
    n = len(logR)
    sum_x = np.sum(logR)
    sum_y = np.sum(logC)
    sum_xy = np.sum(logR * logC)
    sum_x2 = np.sum(logR**2)
    
    D2 = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x**2)
    predicted = D2 * logR + (sum_y - D2 * sum_x) / n
    residuals = logC - predicted
    std = np.sqrt(np.sum(residuals**2) / (n - 2)) * 0.5
    
    return D2, std

# Step 4: Main analysis
def analyze_icecube_data(file_path):
    print("=" * 70)
    print("ICEUBE ENERGY-DEPENDENT Dâ‚‚ ANALYSIS")
    print("Testing Dâ‚‚ â†’ 1.50â€“1.52 at High Energies")
    print("=" * 70)

    # Load data
    data = load_icecube_data(file_path)
    if data is None:
        return
    
    # Classify events into energy bins
    classified = [
        {
            **bin_info,
            'events': data[(data['energy'] >= bin_info['min']) & (data['energy'] < bin_info['max'])]
        }
        for bin_info in energy_bins
    ]
    
    print("\nðŸ“ˆ EVENT DISTRIBUTION:")
    for bin in classified:
        print(f"   {bin['name'].ljust(18)} {bin['range'].ljust(15)} {len(bin['events']):>10,} events")
    
    # Calculate Dâ‚‚ for each bin
    print("\n" + "=" * 70)
    print("Dâ‚‚ MEASUREMENTS BY ENERGY BIN")
    print("=" * 70)
    
    results = []
    for bin in classified:
        if len(bin['events']) >= 500:
            D2, std = calculate_D2(bin['events'])
            if D2 is not None:
                mean_energy = bin['events']['energy'].mean() / 1000  # Convert to TeV
                results.append({
                    'name': bin['name'],
                    'range': bin['range'],
                    'count': len(bin['events']),
                    'mean_energy': mean_energy,
                    'D2': D2,
                    'std': std,
                    'predicted': bin['grok_pred']
                })
                
                print(f"\nðŸ”¬ {bin['name']} ({bin['range']}):")
                print(f"   Events: {len(bin['events']):,}")
                print(f"   Mean Energy: {mean_energy:.2f} TeV")
                print(f"   Dâ‚‚ (measured): {D2:.4f} Â± {std:.4f}")
                print(f"   Dâ‚‚ (predicted): {bin['grok_pred']:.4f}")
                print(f"   Î” = {(D2 - bin['grok_pred']):.4f}")
                print(f"   {'âœ… MATCHES' if abs(D2 - bin['grok_pred']) < 0.1 else 'âš ï¸ DEVIATES'}")
        else:
            print(f"\nâ­ï¸ {bin['name']} ({bin['range']}): {len(bin['events'])} events (insufficient)")
    
    # Analyze trend
    if len(results) >= 2:
        print("\n" + "=" * 70)
        print("THRESHOLD APPROACH ANALYSIS")
        print("=" * 70)
        
        D2_values = [r['D2'] for r in results]
        energies = [r['mean_energy'] for r in results]
        increasing = all(results[i]['D2'] >= results[i-1]['D2'] - 0.1 for i in range(1, len(results)))
        
        print("\nðŸ“ˆ Dâ‚‚ TREND:")
        print(f"   Lowest Energy Dâ‚‚: {min(D2_values):.4f}")
        print(f"   Highest Energy Dâ‚‚: {max(D2_values):.4f}")
        print(f"   Range: {(max(D2_values) - min(D2_values)):.4f}")
        print(f"   Trend: {'âœ… INCREASING' if increasing else 'âŒ NOT increasing'}")
        
        print("\nðŸŽ¯ FRAMEWORK PREDICTIONS:")
        print("   Low Energy (~1 TeV): Dâ‚‚ = 1.49 Â± 0.06")
        print("   High Energy (~10â€“100 TeV): Dâ‚‚ = 1.50 Â± 0.05")
        print("   Ultra-High Energy (>100 TeV): Dâ‚‚ = 1.52 Â± 0.05")
        
        print("\n" + "=" * 70)
        if increasing and max(D2_values) >= 1.48 and max(D2_values) <= 1.54:
            print("âœ… THRESHOLD APPROACH CONFIRMED")
            print("   Dâ‚‚ increases toward 1.50â€“1.52 at high energies")
            print("   Validates framework's tachyonic threshold prediction!")
        elif increasing:
            print("âš ï¸ PARTIAL CONFIRMATION")
            print("   Dâ‚‚ increases with energy, but may not reach 1.50â€“1.52")
        else:
            print("âŒ PREDICTION NOT CONFIRMED")
            print("   Dâ‚‚ does not increase with energy as predicted")
        print("=" * 70)
        
        # Plot Dâ‚‚ vs. energy
        plt.figure(figsize=(10, 6))
        plt.errorbar(energies, D2_values, yerr=[r['std'] for r in results], fmt='o-', label='Measured Dâ‚‚')
        plt.axhline(y=1.49, color='r', linestyle='--', label='Predicted Low (1.49)')
        plt.axhline(y=1.50, color='g', linestyle='--', label='Predicted High (1.50)')
        plt.axhline(y=1.52, color='b', linestyle='--', label='Predicted Ultra-High (1.52)')
        plt.xscale('log')
        plt.xlabel('Mean Energy (TeV)')
        plt.ylabel('Dâ‚‚')
        plt.title('Energy-Dependent Dâ‚‚ Analysis (IceCube Upward Neutrinos)')
        plt.legend()
        plt.grid(True)
        plt.show()

# Step 5: Run the analysis
file_path = 'data.dat'  # Replace with path to IceCube data (e.g., CSV file)
analyze_icecube_data(file_path)