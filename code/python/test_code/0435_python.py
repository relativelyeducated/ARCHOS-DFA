# From: GEEKOM A9 Max AI Performance Overview
# Date: 2025-10-07T21:05:23.460000
# Context: Sounds like you're excited to experiment with the HRM 27M modelâ€”it's a fascinating, lightweight reasoning engine, perfect for a "small model" trial on something like your GEEKOM A9 Max (or cloud). I'l...

# raw_rag.py - Minimal setup (install: pip install faiss-cpu sentence-transformers)
   import torch
   import faiss
   from sentence_transformers import SentenceTransformer
   from hrm_model import HRM  # From repo: load your trained model

   # Load HRM (adapt from pretrain.py)
   model = HRM.from_pretrained('checkpoints/last.ckpt')  # Pseudo; use repo loader
   model.eval()

   # Raw vector DB (embed your "raw" data: e.g., puzzle descriptions or web snippets)
   embedder = SentenceTransformer('all-MiniLM-L6-v2')
   raw_docs = ["Sudoku rule: Fill grid uniquely...", "ARC puzzle: Abstract patterns..."]  # Your raw data
   embeddings = embedder.encode(raw_docs)
   index = faiss.IndexFlatL2(embeddings.shape[1])
   index.add(embeddings.astype('float32'))

   def rag_query(query):
       # Force real-time search/retrieval
       q_emb = embedder.encode([query])
       _, indices = index.search(q_emb.astype('float32'), k=3)  # Top 3 matches
       retrieved = [raw_docs[i] for i in indices[0]]

       # Convert to HRM input (e.g., tokenize to grid/tokens)
       input_grid = tokenize_to_grid(retrieved + [query])  # Custom fn: e.g., grid-ify text

       # Reason with HRM
       with torch.no_grad():
           output = model(input_grid)  # Forward pass for reasoning
       return decode_output(output, retrieved)  # Custom: Interpret as plan/solution

   # Test
   print(rag_query("Solve this Sudoku: [grid desc]"))  # Forces search on every call